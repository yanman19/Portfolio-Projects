I see you're getting a KeyError '2025' when trying to run the `generate_buy_sell_pivot` function with your four pivot tables. This error typically occurs when trying to access a key that doesn't exist in a dictionary or an index that doesn't exist in a DataFrame.

The issue appears to be in how the function is trying to access values from your pivot tables. Looking at line 54 in the error stack trace, it seems to be happening when checking the `hist_raw` values:

```python
# Check hist_raw signal (lower is better for buy)
if pd.notna(hist_df.loc[year, month]):
    hist_value = hist_df.loc[year, month]
```

Let me fix the function to handle this situation better. The main issues could be:

1. Index/column alignment issues between the four pivot tables
2. Styling objects vs actual DataFrames being passed in
3. Handling of missing values

Here's a revised version of the function with better error handling:

```python
import pandas as pd
import numpy as np

def generate_buy_sell_pivot(path_raw, hist_raw, traj_raw, liq_raw):
    """
    Generate a combined Buy/Sell recommendation pivot based on four input pivots.
    
    Parameters:
    path_raw (DataFrame): Percentile pivot (lower is better for buy)
    hist_raw (DataFrame): Percentile pivot (lower is better for buy)
    traj_raw (DataFrame): BULL/BEAR pivot (BULL is buy, BEAR is sell)
    liq_raw (DataFrame): Percentile pivot (lower is better for buy)
    
    Returns:
    pd.io.formats.style.Styler: A styled pivot table with BUY/SELL recommendations and color gradient
    """
    # Extract the underlying DataFrames if we're given Styler objects
    try:
        path_df = path_raw.data if hasattr(path_raw, 'data') else path_raw.copy()
    except:
        path_df = path_raw.copy()
        
    try:
        hist_df = hist_raw.data if hasattr(hist_raw, 'data') else hist_raw.copy()
    except:
        hist_df = hist_raw.copy()
        
    try:
        traj_df = traj_raw.data if hasattr(traj_raw, 'data') else traj_raw.copy()
    except:
        traj_df = traj_raw.copy()
        
    try:
        liq_df = liq_raw.data if hasattr(liq_raw, 'data') else liq_raw.copy()
    except:
        liq_df = liq_raw.copy()
    
    # Print information about the DataFrames for debugging
    print(f"path_df shape: {path_df.shape}, index: {path_df.index.tolist()}, columns: {path_df.columns.tolist()}")
    print(f"hist_df shape: {hist_df.shape}, index: {hist_df.index.tolist()}, columns: {hist_df.columns.tolist()}")
    print(f"traj_df shape: {traj_df.shape}, index: {traj_df.index.tolist()}, columns: {traj_df.columns.tolist()}")
    print(f"liq_df shape: {liq_df.shape}, index: {liq_df.index.tolist()}, columns: {liq_df.columns.tolist()}")
    
    # Find common indices and columns
    common_index = sorted(set(path_df.index) & set(hist_df.index) & set(traj_df.index) & set(liq_df.index))
    common_columns = sorted(set(path_df.columns) & set(hist_df.columns) & set(traj_df.columns) & set(liq_df.columns))
    
    print(f"Common index: {common_index}")
    print(f"Common columns: {common_columns}")
    
    # Create an empty dataframe to store the combined results
    result_df = pd.DataFrame(index=common_index, columns=common_columns)
    
    # Create an empty dataframe to store the score values
    score_df = pd.DataFrame(index=common_index, columns=common_columns)
    
    # Initialize result_df and score_df with NaN values
    result_df[:] = np.nan
    score_df[:] = np.nan
    
    # Iterate through each cell in the result dataframe
    for year in common_index:
        for month in common_columns:
            # Initialize counters for buy and sell signals
            buy_signals = 0
            sell_signals = 0
            
            # Initialize score (0 = neutral, negative = buy, positive = sell)
            score = 0
            signal_count = 0
            
            # Check path_raw signal (lower is better for buy)
            try:
                if year in path_df.index and month in path_df.columns and pd.notna(path_df.loc[year, month]):
                    path_value = float(path_df.loc[year, month])
                    signal_count += 1
                    
                    # Convert to a score between -5 (strong buy) and +5 (strong sell)
                    path_score = (path_value - 50) / 10  # Scale from -5 to +5
                    score += path_score
                    
                    if path_value < 50:
                        buy_signals += 1
                    else:
                        sell_signals += 1
            except Exception as e:
                print(f"Error processing path_df at {year}, {month}: {e}")
            
            # Check hist_raw signal (lower is better for buy)
            try:
                if year in hist_df.index and month in hist_df.columns and pd.notna(hist_df.loc[year, month]):
                    hist_value = float(hist_df.loc[year, month])
                    signal_count += 1
                    
                    # Convert to a score between -5 (strong buy) and +5 (strong sell)
                    hist_score = (hist_value - 50) / 10  # Scale from -5 to +5
                    score += hist_score
                    
                    if hist_value < 50:
                        buy_signals += 1
                    else:
                        sell_signals += 1
            except Exception as e:
                print(f"Error processing hist_df at {year}, {month}: {e}")
            
            # Check traj_raw signal (BULL is buy, BEAR is sell)
            try:
                if year in traj_df.index and month in traj_df.columns and pd.notna(traj_df.loc[year, month]):
                    traj_value = str(traj_df.loc[year, month])
                    signal_count += 1
                    
                    if traj_value == "BULL":
                        buy_signals += 1
                        score -= 2.5  # Add a fixed buy score
                    elif traj_value == "BEAR":
                        sell_signals += 1
                        score += 2.5  # Add a fixed sell score
            except Exception as e:
                print(f"Error processing traj_df at {year}, {month}: {e}")
            
            # Check liq_raw signal (lower is better for buy)
            try:
                if year in liq_df.index and month in liq_df.columns and pd.notna(liq_df.loc[year, month]):
                    liq_value = float(liq_df.loc[year, month])
                    signal_count += 1
                    
                    # Convert to a score between -5 (strong buy) and +5 (strong sell)
                    liq_score = (liq_value - 50) / 10  # Scale from -5 to +5
                    score += liq_score
                    
                    if liq_value < 50:
                        buy_signals += 1
                    else:
                        sell_signals += 1
            except Exception as e:
                print(f"Error processing liq_df at {year}, {month}: {e}")
            
            # Only process if we have enough signals
            if signal_count >= 1:
                # Store the overall score
                score_df.loc[year, month] = score
                
                # Determine the final recommendation based on the signals
                if buy_signals >= 3 and buy_signals > sell_signals:
                    result_df.loc[year, month] = "BUY"
                elif sell_signals >= 3 and sell_signals > buy_signals:
                    result_df.loc[year, month] = "SELL"
                else:
                    result_df.loc[year, month] = "-"  # Neutral or conflicting signals
    
    # Create a function to apply color formatting based on the score
    def color_format(val, score):
        """Apply color formatting based on the recommendation and score strength"""
        if pd.isna(val):
            return ""
            
        if val == "BUY":
            # Scale from light green (weak buy) to dark green (strong buy)
            # Score will be negative for buys, stronger buy = more negative
            intensity = min(100, max(0, abs(score) * 10))
            
            # Create green gradient from light to dark
            r = max(0, 50 - intensity * 0.5)
            g = max(100, 150 - intensity * 0.5)
            b = max(0, 50 - intensity * 0.5)
            
            return f"background-color: rgb({int(r)}, {int(g)}, {int(b)}); color: white; font-weight: bold"
            
        elif val == "SELL":
            # Scale from light red (weak sell) to dark red (strong sell)
            intensity = min(100, max(0, abs(score) * 10))
            
            # Create red gradient from light to dark
            r = max(100, 150 - intensity * 0.5)
            g = max(0, 50 - intensity * 0.5)
            b = max(0, 50 - intensity * 0.5)
            
            return f"background-color: rgb({int(r)}, {int(g)}, {int(b)}); color: white; font-weight: bold"
            
        else:  # Neutral ("-")
            return "background-color: #CCCCCC; color: black"
    
    # Define a function to apply styling row by row
    def style_row(row):
        styles = []
        for col, val in row.items():
            if pd.isna(val):
                styles.append("")
            else:
                score = score_df.loc[row.name, col] if pd.notna(score_df.loc[row.name, col]) else 0
                styles.append(color_format(val, score))
        return styles
    
    # Apply the color formatting
    styled_result = result_df.style.apply(style_row, axis=1)
    
    return styled_result
```

Key improvements:

1. **Better DataFrame Extraction**: More robust handling of Styler objects versus DataFrames.

2. **Debugging Information**: Prints shapes, indices, and columns of input tables to diagnose issues.

3. **Common Elements**: Finds common indices and columns across all four pivots to ensure we only work with cells that exist in all tables.

4. **Error Handling**: Adds try/except blocks around each lookup to catch and report specific errors.

5. **Type Safety**: Explicitly converts values to the right type (float for numeric values, str for BULL/BEAR).

6. **Existence Checks**: Verifies that indices and columns exist before attempting to access them.

7. **Conditional Processing**: Only processes cells with at least one signal.

This modified function should handle the misalignments in your pivot tables and provide better error reporting to help troubleshoot any remaining issues.​​​​​​​​​​​​​​​​